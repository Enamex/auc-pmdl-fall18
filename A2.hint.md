Concepts that you've studied so far which you'll probably need:

### Functional (in-model) Concepts:

- Functional layers: layers are just pure functions, they depend only on their input (no globals access or changing of inputs in a way visible to the caller) to give a new value output.
- This applies to linear/dense/fully-connected layers (the multiply-and-sum operation on weights and inputs) and activation layers (element-wise activation on inputs). This can make things simpler (i.e. separate the linear operation from activations)
    - These layers can be queried for 3 values:
        - [1] Given input X, produce output Y (this just gives the output)
            f(X; W) = Y
        - [2] Given input X, produce the gradient of output Y with respect to X (this gives us the partial derivative 'delta' that we cache for use in the preceding layer as we go further back in back-prop)
            df/dX(X; W) = ?
        - [3] Given input X, produce the gradient of output Y with respect to the hidden parameters W (this gives us the partial derivative that we use in our update rule)
            df/dW(X; W) = ?
- Layer composition: exactly composition of plain old functions. So (L3 . L2 . L1)(X) applies the layers 1 through 3 to the input X. This's also called layer sequencing or concatenation.

### Procedural (out-of-model) Concepts: These processes should be possible to write just once and they'd work on any layer you design as long as all layers satisfy the interface these processes expect (basically providing [1, 2, 3])
- Forward pass: Just the application of every layer ('s function) on the output of the preceding layer and passing on the output to the succeeding layer.
- Backward pass:
    - Using [3], calculate the derivative with respect to the hidden parameters and store it for the update
    - Using [2], calculate the derivative with respect to the input to the layer and pass it along to the preceding layer so it can continue the chain calculation

Example:

```py
def forward(layers, X) -> (Prediction, LayerOuts):
  Ys = []
  for lyr in layers:
    Y = lyr.forward(X)
    X = Y
    Ys.append(Y)
  return Y, Ys

def backward(layers: List[Layer], X: Inputs, Y: Predictons, Ys: LayerOuts):
  dW = []
  dX = ones_like(Y)
  for n reversed(range(1, len(layers))):
    lyr = layers[n]
    # Ys[n] == Xs[n+1] ; Y_n == X_(n+1)
    dW0 = lyr.grad_to_params(Ys[n-1], dX) #[3]== dL/dWn
    dX0 = lyr.grad_to_input(Ys[n-1])      #[2]== dYn/dXn
    dX  = lyr.grad_loss_to_input(dX, dX0) # appropriate multiplication based on shape
                                          #== delta == (dL/dYn) dot (dYn/dXn) == dL/dXn
    dW.append(dW0)
  update(layers, dW) # mutate the model to apply the updates
                     # This assumes SGD; you'll need some changes to support minibatches

```